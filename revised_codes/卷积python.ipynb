{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3db82824-8e02-454f-9a9d-f2c7caee073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1c96862a-0ac2-4e5b-8da6-359c40c06aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = torch.tensor([0, 1], dtype=torch.float32)\n",
    "normal_matrix1 = torch.randn(4, 4)\n",
    "normal_matrix2 = torch.randn(4, 4)\n",
    "matrix = torch.stack([normal_matrix1,\n",
    "                     normal_matrix2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "53641764-f55a-4b6f-b9ef-6b32814fe10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##定义卷积层\n",
    "class Convol_2(nn.Module):\n",
    "    def __init__(self , out_channels , in_channels , kernel_sizex , kernel_sizey):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizex , self.kernel_sizey = kernel_sizex , kernel_sizey # kernel_size为 (x,y) 数对\n",
    "        self.weight = nn.Parameter(torch.randn( out_channels , in_channels , kernel_sizex , kernel_sizey ) )\n",
    "        ##randn 生成标准正态分布数据\n",
    "        # self.bias = nn.Parameter(torch.randn(out_channels) )\n",
    "    '''    \n",
    "    def forward(self,X):\n",
    "        return F.conv2d(X,self.weight,self.bias,padding = 1)\n",
    "    '''\n",
    "    \n",
    "    ##具体实现\n",
    "\n",
    "    ## 2d实现(直接卷积)\n",
    "    def corr2d(self,X,K):\n",
    "        h , w = K.shape\n",
    "        Y = torch.zeros(X.shape[0] - h + 1,X.shape[1] - w + 1)\n",
    "        for i in range(Y.shape[0]):\n",
    "            for j in range(Y.shape[1]):\n",
    "                Y[i,j] = (X[i:i+h,j:j+w] * K).sum() ## * 代表点积 ，即每个对应元素相乘\n",
    "        return Y\n",
    "    \n",
    "    def multi_in_channels(self,X,K):\n",
    "        #错误示范\n",
    "        '''\n",
    "        return torch.sum(torch.stack([corr2d(X, k) for k in K]), dim = 0) # stack 对 最外层，也就是shape[0]进行遍历\n",
    "        #return torch.sum(torch.stack([corr2d(X,k) for k in K])) # k是遍历K的最外层取出的矩阵，n-1维度\n",
    "        '''\n",
    "        #正确示范\n",
    "        return sum(self.corr2d(x, k) for x, k in   zip(X, K)) # zip (X,K) X,K外面的维度做遍历\n",
    "\n",
    "        #zip(X,K) 将第一维度配对，迭代返回X,K的配对后两维数据\n",
    "\n",
    "    def multi_out_channels(self,X,K):\n",
    "        return torch.stack([self.multi_in_channels(X,k) for k in K],dim = 0)\n",
    "        #不加sum的话，所有结果都叠加起来\n",
    "        #因为输出通道不用叠加\n",
    "\n",
    "    def forward(self,X):\n",
    "        # return self.multi_out_channels(X,self.weight)+self.bias.reshape(self.out_channels,1,1)\n",
    "        return self.multi_out_channels(X,self.weight)\n",
    "\n",
    "def corr2d(X,K):\n",
    "    h , w = K.shape\n",
    "    Y = torch.zeros(X.shape[0] - h + 1,X.shape[1] - w + 1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (X[i:i+h,j:j+w] * K).sum() ## * 代表点积 ，即每个对应元素相乘\n",
    "    return Y\n",
    "def multi_in_channels(X,K):\n",
    "    return sum(corr2d(x, k) for x, k in   zip(X, K)) # zip (X,K) X,K外面的维度做遍历\n",
    "def multi_out_channels(X,K):\n",
    "    return torch.stack([multi_in_channels(X,k) for k in K],dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4457ae84-bc1c-4c35-b78a-459817450149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2695, -0.9194,  1.5498],\n",
      "         [-2.2975, -1.7072, -0.5453],\n",
      "         [-1.0065, -1.5351, -1.1163]],\n",
      "\n",
      "        [[-1.2695, -0.9194,  1.5498],\n",
      "         [-2.2975, -1.7072, -0.5453],\n",
      "         [-1.0065, -1.5351, -1.1163]]])\n"
     ]
    }
   ],
   "source": [
    "W = torch.ones(2,1,2,2)\n",
    "print(multi_out_channels(matrix,W))\n",
    "Y = multi_out_channels(matrix,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1b7fc1a2-8b98-4a64-ba2f-feb1f67763b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "myconv = Convol_2(2,1,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "01726e8f-be00-4556-95c5-e25b8572680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 20, loss 70.444\n",
      "batch 40, loss 49.325\n",
      "batch 60, loss 35.954\n",
      "batch 80, loss 27.149\n",
      "batch 100, loss 21.104\n",
      "batch 120, loss 16.780\n",
      "batch 140, loss 13.571\n",
      "batch 160, loss 11.113\n",
      "batch 180, loss 9.182\n",
      "batch 200, loss 7.635\n",
      "batch 220, loss 6.377\n",
      "batch 240, loss 5.344\n",
      "batch 260, loss 4.489\n",
      "batch 280, loss 3.777\n",
      "batch 300, loss 3.182\n",
      "batch 320, loss 2.684\n",
      "batch 340, loss 2.265\n",
      "batch 360, loss 1.913\n",
      "batch 380, loss 1.617\n",
      "batch 400, loss 1.367\n",
      "batch 420, loss 1.157\n",
      "batch 440, loss 0.979\n",
      "batch 460, loss 0.829\n",
      "batch 480, loss 0.702\n",
      "batch 500, loss 0.595\n",
      "batch 520, loss 0.504\n",
      "batch 540, loss 0.428\n",
      "batch 560, loss 0.363\n",
      "batch 580, loss 0.308\n",
      "batch 600, loss 0.262\n",
      "batch 620, loss 0.222\n",
      "batch 640, loss 0.189\n",
      "batch 660, loss 0.161\n",
      "batch 680, loss 0.137\n",
      "batch 700, loss 0.116\n",
      "batch 720, loss 0.099\n",
      "batch 740, loss 0.084\n",
      "batch 760, loss 0.072\n",
      "batch 780, loss 0.061\n",
      "batch 800, loss 0.052\n",
      "batch 820, loss 0.045\n",
      "batch 840, loss 0.038\n",
      "batch 860, loss 0.033\n",
      "batch 880, loss 0.028\n",
      "batch 900, loss 0.024\n",
      "batch 920, loss 0.020\n",
      "batch 940, loss 0.017\n",
      "batch 960, loss 0.015\n",
      "batch 980, loss 0.013\n",
      "batch 1000, loss 0.011\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    Y_hat = myconv(matrix)\n",
    "    l = (Y_hat - Y)**2 # loss\n",
    "    myconv.zero_grad() #清零梯度\n",
    "    l.sum().backward() # 记录梯度\n",
    "    myconv.weight.data[:] -= 5e-4 * myconv.weight.grad # 学习率乘以 -grad,每一个weight data都是\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f'batch {i+1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e6ae17-c6be-49c6-b96f-04268718d360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1e107-0349-4744-9a68-7df3d138176b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
